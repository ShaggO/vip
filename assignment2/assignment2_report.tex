\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{subcaption}

\usepackage{amsmath,amssymb,amsfonts,adjustbox,caption}

\title{Vision and Image Processing\\Assignment 2}
\author{Malte St√¶r Nissen \\ \texttt{tgq958} \and Benjamin Braithwaite \\
\texttt{cpg608}}

\begin{document}
\maketitle

\section{General notes}
We have implemented the content based image retrieval (CBIR) of this assignment
in Matlab 2013b. Furthermore we are using the VLFeat library (v. 0.9.17). We
have chosen to use the CalTech 101 image database for testing out the
performance of our CBIR implementation. From this image database we choose a
training part and a test part. These parts are specified later when we test the
performance.

\section{Codebook generation}
The first step of our CBIR system is to create a code book for our images to
compare test images against.

We do this by extracting SIFT features for each of the training images. All
these features are combined in a matrix where each row corresponds to a
feature. We now choose a number $k$ of clusters (words) and perform clustering
of the features resulting in $k$ cluster centers or words followed by creating
a bag of words for each image by counting the number of features (of the image)
associated to each word.

\section{Indexing}
The second step of our CBIR system is to perform indexing of the test dataset
using the newly generated code book. Once again we first extract the SIFT
features of each of the test images and place these in a combined matrix. We
then project each of the descriptors onto the cluster centers of our code book
and hence get a mapping for each of the features to each of the words. Counting
the number of features related to each word for each image gives us the bag of
words for each image. The information is combined in a cell array where each
entry corresponds to an image and contains the image path, correct category and
bag of words where each entry corresponds to an image and contains the image
path, correct category and bag of words..

\section{Retrieving}
The final step of our CBIR system is to retrieve the category of each test
image. This is done by comparing the bag of words of each test image with the
bag of words for each of the train images. This comparison can be done using
various types of (dis)similarity measures. We have chosen to test the
Bhattacharyya measure, the Euclidean distance, and the
Kullback-Leibler divergence (symmetric version). We simply choose the category
of each test image as the category of the best matching train image using one of
the previously mentioned measures.

\section{Results}

\subsection{Input parameters}

We have decided to use all 101 categories in the image dataset, and take out $n_{train}$ training images and $n_{test}$ test images from each category. This means if were to randomly guess the category of the test images, we would get an accuracy of just under $0.01$. When we use clustering to generate the words, we only use a 10th of the SIFT features resulting from VLFeat, as it would otherwise be too slow. However, we use all the features computing the bag of words for each image.

The parameters $k$ and $n_{train}$ are important for the accuracy of our CBIR system. Although the best value for $k$ is dependent on $n_{train}$ and vice versa, since the CBIR process is quite time consuming, we simply optimize the parameters separately.

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/results_k.pdf}
        \caption{Matching accuracy for $n_{train} = 5$ and varying $k$.}
        \label{fig:results_k}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/results_k_time.pdf}
        \caption{Computation time for $n_{train} = 5$ and varying $k$.}
        \label{fig:results_k_time}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/results_n_train.pdf}
        \caption{Matching accuracy for $k = 2500$ and varying $n_{train}$.}
        \label{fig:results_n_train}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\textwidth}
        \includegraphics[width=\textwidth]{images/results_n_train_time.pdf}
        \caption{Computation time for $k = 2500$ and varying $n_{train}$.}
        \label{fig:results_n_train_time}
    \end{subfigure}
    \caption{Parameter study results for training and testing the
    implementation}
    \label{fig:parameter_results}
\end{figure}

\subsection{Examples of resulting retrieving}
In this subsection we look at a couple of examples of the output of the
retrieving process.
All the examples we present in this subsection are generated using the best
parameters described in the previous subsection.

Figure \ref{fig:results_11} shows a test image (left-most)
and the corresponding three best matches (best ordered left to right). The
image is correctly categorized as a beaver. Looking at the images the content
of the two matched images is quite different but the algorithm succeeds
nonetheless. The distance between the three best matches is however quite
minimal (between $d = 0.7050$ and $d = 0.7177$). It should be noted that the
Bhattacharyya similarity measure has been ``inverted'' by subtracting it from 1
giving us the distances $d$.
The histogram of each image's bag of words is shown below their
corresponding image. These histograms look quite different to the human eye,
but the use of the Bhattacharyya similarity give them nearly the same score as
mentioned earlier. Looking at the actual matched training images we see that
there is quite a large difference in the images (a woodden chair and a
butterfly) compared with the test image (a beaver) and hence the close matching
is surprising from a human perspective.

Figure \ref{fig:results_18} is another example of a retrieving. This example
has however failed to perform a correct categorization. Once again the test
image is shown as the left-most image and the best (left to right) three
matches are shown in the figure as well. By looking at the three matches we
see that the second-best match would have given a correct categorization.
Again the distances $d$ are quite close (between 0.8157 and 0.8223). Once
again the histograms of the bag of words shown below each image are of little
help when performing visual inspection to try to justify the failed or
successful matches. When looking at the actual images from a human perspective
the two train images (laptop and pidgeon) of wrong categories are however
quite different from the correct one (camera).
\begin{figure}[H]
\centering
\adjincludegraphics[scale=.4]{images/results_11.pdf}
\caption{Example of successful image matching: test image with its 3 best
    training image matches, along with their bag of words histograms.}
\label{fig:results_11}
\end{figure}
%
\begin{figure}[H]
\centering
\adjincludegraphics[scale=.4]{images/results_18.pdf}
\caption{Example of failed image matching: test image with its 3 best training
    image matches, along with their bag of words histograms.}
\label{fig:results_18}
\end{figure}
%
\end{document}
